vinos <- read.csv2("winequality-red.csv", dec = ".")

names(vinos)


library(dplyr)

#Escalamos el modelo
modelo <- lm(quality~., data = vinos )
summary(modelo)

#Estadísticamente significativas:
#volatile.acidity / chlorides /free.sulfur.dioxide /total.sulfur.dioxide /pH /sulphates /alcohol

#Limpiamos el dataset con un pipeline asignando las variables mencionadas antes

vinos_reducido <- vinos[c("quality", "volatile.acidity", "chlorides",
                          "free.sulfur.dioxide", "total.sulfur.dioxide",
                          "pH", "sulphates", "alcohol")]


#----------------------------------------------------------------------------------------#
#manera de hacerlo según el libro donde define algunos parámetros mas explicitos

library(leaps)
modelo1 <- regsubsets(quality~ ., vinos, nvmax = 11)
bestmodelo1 <- summary(modelo1)


names(bestmodelo1)
# "which"  "rsq"    "rss"    "adjr2"  "cp"     "bic"    "outmat" "obj"   

bestmodelo1$rsq
#0.2267344 0.3170024 0.3358973 0.3437824 0.3514942 0.3571736 0.3594709 0.3599265 0.3601728
#[10] 0.3602764 0.3605517

#Notamos que a mayor cantidad de variables mejora el Rsquared pero llegando a las últimas variables 
#estanca su valor ya no siendo tan significativa la agregacion de las mismas


#Graficamos para observar lo que esta pasando
par(mfrow = c(1, 1))

#Vemos la suma de los residuos al cuadrado y cómo disminuye al aumentar variables
plot(bestmodelo1$rss, xlab = "Number of Variables",
       ylab = "RSS", type = "l")

#Rsquared ajustado y aumenta su relacion al agregar varables pero sobre el final cae levemente
plot(bestmodelo1$adjr2, xlab = "Number of Variables",
       ylab = "Adjusted RSq", type = "l")

#Se corrobora la seleccion de las variables estadisticamente más significativas

#--------------------------------------------------------------------------------
# Predicciones y particion del dataset

library(glmnet)

set.seed(123)
particion <- createDataPartition(vinos_reducido$quality, p = 0.8, list = FALSE )
train_set <- vinos_reducido[particion, ]
test_set <- vinos_reducido[-particion, ]

#Matrices para Ridge y Lasso

#glmnet requiere que las variables predictoras estén en formato de matriz y la variable respuesta 
#como vector numérico.

x_train <- model.matrix(quality ~ . , data = train_set)[, -1]  
y_train <- train_set$quality

x_test <- model.matrix(quality ~ . , data = test_set)[, -1]  
y_test <- test_set$quality

#definimos modelos con las variables finales para luego establecer predicciones
modelo.lineal <- lm(quality ~ ., data = train_set)

# Mejor lambda, el que minimiza el MSE

ridge.model <- glmnet(x_train, y_train, alpha = 0)
ridgelambda <- cv.glmnet(x_train, y_train, alpha = 0)$lambda.min

lasso.model <- glmnet(x_train, y_train, alpha = 1)
lassolambda <- cv.glmnet(x_train, y_train, alpha = 1)$lambda.min  
  
pred.ridge <- predict(ridge.model, s = ridgelambda, newx = x_test)
pred.lasso <- predict(lasso.model, s = lassolambda, newx = x_test)
pred.lineal <- predict(modelo.lineal, newdata = test_set)


# RMSE
rmse_ridge <- sqrt(mean((y_test - pred.ridge)^2))
rmse_lasso <- sqrt(mean((y_test - pred.lasso)^2))
rmse_lineal <- sqrt(mean((y_test - pred.lineal)^2))

print("La Raíz del Error Cuadrático Medio para Ridge es:")
rmse_ridge

print("La Raíz del Error Cuadrático Medio para Lasso es: ")
rmse_lasso

print("La Raíz del Error Cuadrático Medio para el Modelo Lineal es:")
rmse_lineal

#El mejor resultado obtenido es 0.6466593 por parte de Lasso
